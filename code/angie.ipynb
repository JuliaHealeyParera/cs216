{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloaded datasets from https://www.sports-reference.com/cbb/schools/connecticut/men/2025.html#all_roster with two csv's per team, one of roster for year (upperclassmen) and one of per game for starters, with team totals in same table having info on the field goals, free throws, etc aka common bbal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 2024-25 uconn.html\n",
      "Processing table with div ID: all_roster in team 2024-25 uconn\n",
      "Headers: ['Player', '#', 'Class', 'Pos', 'Height', 'Weight', 'Hometown', 'High School', 'RSCI Top 100', 'Summary']\n",
      "Table 'all_roster' for team '2024-25 uconn' saved to ../2024-25 uconn_all_roster.csv\n",
      "Processing table with div ID: all_advanced_players in team 2024-25 uconn\n",
      "Headers: ['Rk', 'Player', 'G', 'GS', 'MP', 'PER', 'TS%', 'eFG%', '3PAr', 'FTr', 'PProd', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', '', 'OWS', 'DWS', 'WS', 'WS/40', '', 'OBPM', 'DBPM', 'BPM']\n",
      "Table 'all_advanced_players' for team '2024-25 uconn' saved to ../2024-25 uconn_all_advanced_players.csv\n"
     ]
    }
   ],
   "source": [
    "#getting from downloaded html file\n",
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the HTML files\n",
    "html_folder = '../data/html_sportsreference'\n",
    "output_folder = '../'  # Directory to save CSV files\n",
    "\n",
    "# List of table div IDs to extract\n",
    "table_div_ids = ['all_roster', 'all_advanced_players']\n",
    "\n",
    "# Loop through each HTML file in the folder\n",
    "for filename in os.listdir(html_folder):\n",
    "    if filename.endswith(\".html\"):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        file_path = os.path.join(html_folder, filename)\n",
    "\n",
    "        # Open and parse the HTML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "        # Extract team name from filename (without .html)\n",
    "        team_name = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Loop through each table ID\n",
    "        for div_id in table_div_ids:\n",
    "            print(f\"Processing table with div ID: {div_id} in team {team_name}\")\n",
    "            div = soup.find('div', id=div_id)\n",
    "            if div:\n",
    "                table = div.find('table')  # Locate the table inside the div\n",
    "                if table:\n",
    "                    # Extract headers from <thead>\n",
    "                    thead = table.find('thead')\n",
    "                    headers = [header.text.strip() for header in thead.find_all('th')] if thead else []\n",
    "                    print(\"Headers:\", headers)\n",
    "\n",
    "                    # Extract rows from <tbody>\n",
    "                    tbody = table.find('tbody')\n",
    "                    rows = []\n",
    "                    if tbody:\n",
    "                        for row in tbody.find_all('tr'):\n",
    "                            cells = row.find_all('td')\n",
    "                            row_data = [cell.text.strip() for cell in cells]\n",
    "                            # Check if the row length matches headers\n",
    "                            if len(row_data) == len(headers):\n",
    "                                rows.append(row_data)\n",
    "                            else:\n",
    "                                #print(f\"Row length mismatch in table '{div_id}':\", row_data)\n",
    "                                # Pad the row to match headers\n",
    "                                row_data.extend([''] * (len(headers) - len(row_data)))\n",
    "                                rows.append(row_data)\n",
    "\n",
    "                    # Convert to DataFrame\n",
    "                    if rows:\n",
    "                        df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "                        # Save to CSV with team name and table ID\n",
    "                        csv_filename = f\"{team_name}_{div_id}.csv\"\n",
    "                        csv_path = os.path.join(output_folder, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Table '{div_id}' for team '{team_name}' saved to {csv_path}\")\n",
    "                    else:\n",
    "                        print(f\"No valid rows found for table '{div_id}' in team '{team_name}'\")\n",
    "                else:\n",
    "                    print(f\"No table found with ID '{div_id}' for team '{team_name}'\")\n",
    "            else:\n",
    "                print(f\"No div found with ID '{div_id}' for team '{team_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing team name: connecticut\n",
      "../connecticut_all_roster.csv\n",
      "Table 'all_roster' for team 'connecticut' saved to ../connecticut_all_roster.csv\n",
      "../connecticut_all_advanced_players.csv\n",
      "Table 'all_advanced_players' for team 'connecticut' saved to ../connecticut_all_advanced_players.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#same as above, but using direct url instead \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    'https://www.sports-reference.com/cbb/schools/connecticut/men/2023.html',\n",
    "    # Add more URLs here\n",
    "]\n",
    "\n",
    "# Directory to save CSV files\n",
    "output_folder = '../'\n",
    "\n",
    "# List of table div IDs to extract\n",
    "table_div_ids = ['all_roster', 'all_advanced_players']\n",
    "\n",
    "# Loop through URLs\n",
    "for url in urls:\n",
    "    #print(f\"Processing URL: {url}\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract team name from the URL\n",
    "        team_name = url.split('/')[-3]  # Extract team name from URL path\n",
    "        print(f\"Processing team name: {team_name}\")\n",
    "\n",
    "        # Loop through each table ID\n",
    "        for div_id in table_div_ids:\n",
    "            #print(f\"Processing table with div ID: {div_id} for team {team_name}\")\n",
    "            div = soup.find('div', id=div_id)\n",
    "            if div:\n",
    "                table = div.find('table')  # Locate the table inside the div\n",
    "                if table:\n",
    "                    # Extract headers from <thead>\n",
    "                    thead = table.find('thead')\n",
    "                    headers = [header.text.strip() for header in thead.find_all('th')] if thead else []\n",
    "                    \n",
    "                    # Extract rows from <tbody>\n",
    "                    tbody = table.find('tbody')\n",
    "                    rows = []\n",
    "                    if tbody:\n",
    "                        for row in tbody.find_all('tr'):\n",
    "                            cells = row.find_all('td')\n",
    "                            row_data = [cell.text.strip() for cell in cells]\n",
    "                            # Check if the row length matches headers\n",
    "                            if len(row_data) == len(headers):\n",
    "                                rows.append(row_data)\n",
    "                            else:\n",
    "                                #print(f\"Row length mismatch in table '{div_id}':\", row_data)\n",
    "                                # Pad the row to match headers\n",
    "                                row_data.extend([''] * (len(headers) - len(row_data)))\n",
    "                                rows.append(row_data)\n",
    "\n",
    "                    # Convert to DataFrame\n",
    "                    if rows:\n",
    "                        df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "                        # Save to CSV with team name and table ID\n",
    "                        csv_filename = f\"{team_name}_{div_id}.csv\"\n",
    "                        csv_path = os.path.join(output_folder, csv_filename)\n",
    "                        df.to_csv(csv_path, index=False)\n",
    "                        print(f\"Table '{div_id}' for team '{team_name}' saved to {csv_path}\")\n",
    "                    else:\n",
    "                        print(f\"No valid rows found for table '{div_id}' for team '{team_name}'\")\n",
    "                else:\n",
    "                    print(f\"No table found with ID '{div_id}' for team '{team_name}'\")\n",
    "            else:\n",
    "                print(f\"No div found with ID '{div_id}' for team '{team_name}'\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}, Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'basic_school_stats' saved to ../basic_school_stats.csv\n"
     ]
    }
   ],
   "source": [
    "#all school stats for regular bbal stuff\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the local HTML file\n",
    "file_path = '../data/html_teamranking/all schools 2022 season.html'\n",
    "\n",
    "# Open and parse the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Locate the table\n",
    "div_id = 'div_basic_school_stats'\n",
    "table = soup.find('div', id=div_id).find('table', id='basic_school_stats')\n",
    "\n",
    "if table:\n",
    "    # Extract headers from the table's <thead>\n",
    "    headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "\n",
    "    # Extract rows from the table's <tbody>\n",
    "    rows = []\n",
    "    for row in table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        # Ensure the row has the same number of columns as headers\n",
    "        if len(row_data) < len(headers):\n",
    "            row_data.extend([''] * (len(headers) - len(row_data)))  # Pad missing cells with blanks\n",
    "        rows.append(row_data[:len(headers)])  # Trim any extra cells\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        csv_filename = '../basic_school_stats.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Table 'basic_school_stats' saved to {csv_filename}\")\n",
    "    else:\n",
    "        print(\"No valid rows found in the table.\")\n",
    "else:\n",
    "    print(\"Table 'basic_school_stats' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
